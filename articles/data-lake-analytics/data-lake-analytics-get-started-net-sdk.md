<properties 
   pageTitle="透過 .NET SDK 開始使用 Azure 資料湖分析 | Azure" 
   description="了解如何透過 .NET SDK 建立資料湖存放區帳戶、建立資料湖分析工作，以及提交以 U-SQL 撰寫的工作。 " 
   services="data-lake-analytics" 
   documentationCenter="" 
   authors="edmacauley" 
   manager="paulettm" 
   editor="cgronlun"/>

<tags
   ms.service="data-lake-analytics"
   ms.devlang="na"
   ms.topic="article"
   ms.tgt_pltfrm="na"
   ms.workload="big-data" 
   ms.date="10/21/2015"
   ms.author="edmaca"/>


# 教學課程：透過 .NET SDK 開始使用 Azure 資料湖分析

[AZURE.INCLUDE [get-started-selector](../../includes/data-lake-analytics-selector-get-started.md)]


了解如何使用 Azure.NET SDK 來建立 Azure 資料湖分析帳戶、 定義資料湖分析
中的工作 [U SQL](data-lake-analytics-u-sql-get-started.md), ，將作業提交至資料湖 Analtyic 帳戶。 如需 
資料湖分析的相關資訊，請參閱 [Azure 資料湖分析概觀](data-lake-analytics-overview.md)。

在本教學課程中，您將開發包含讀取定位鍵分隔值 (TSV) 檔案，並將它轉換成逗號的 U SQL 指令碼的 C# 主控台應用程式 
分隔的值 (CSV) 檔案。 若要使用其他支援的工具進行同一個教學課程，請按一下此區段最上方的索引標籤。

**基本資料湖分析程序：**

![Azure 資料湖分析程序流程圖](./media/data-lake-analytics-get-started-portal/data-lake-analytics-process.png)

1. 建立資料湖分析帳戶。
2. 準備來源資料。 資料湖分析工作可讀取 Azure 資料湖存放區帳戶或 Azure Blob 儲存體帳戶資料中的資料。
3. 開發 U SQL 指令碼。
4. 將工作 (U-SQL 指令碼) 提交至資料湖分析帳戶。 作業會讀取來源資料，處理資料的指示 
U SQL 指令碼，然後再儲存輸出資料湖存放區帳戶或 Blob 儲存體帳戶。

## 必要條件

開始進行本教學課程之前，您必須具備下列條件：

- **已安裝 Visual Studio 2015、Visual Studio 2013 更新 4，或具有 Visual C++ 的 Visual Studio 2012**。
- **Microsoft Azure SDK for .NET 2.5 版或更新版本**。 請使用 [Web platform installer](http://www.microsoft.com/web/downloads/platform.aspx)。
- * *[資料湖 Tools for Visual Studio](http://aka.ms/adltoolsvs)* *。
- **資料湖分析帳戶**。 請參閱 [建立 Azure 資料湖分析帳戶](data-lake-analytics-get-started-portal.md#create_adl_analytics_account)。

    資料湖工具不支援建立資料湖分析帳戶。 因此您必須使用 Azure 入口網站、Azure PowerShell、.NET SDK 或 Azure CLI 建立帳戶。

## 建立主控台應用程式

在本教學課程中，您將會處理一些搜尋記錄檔。 搜尋記錄檔可以儲存在資料湖存放區或 Azure Blob 儲存體中。

系統已將搜尋記錄檔範例複製到公用 Azure Blob 容器。 在應用程式中，您會將該檔案下載至您的工作站，然後將檔案上傳到預設的資料湖存放區帳戶。

**若要建立應用程式**

1. 開啟 Visual Studio。
2. 建立 C# 主控台應用程式。
3. 開啟 Nuget 封裝管理主控台，然後執行下列命令：

        Install-Package Microsoft.Azure.Common.Authentication -Pre
        Install-Package Microsoft.Azure.Management.DataLake.Analytics -Pre
        Install-Package Microsoft.Azure.Management.DataLake.AnalyticsCatalog -Pre
        Install-Package Microsoft.Azure.Management.DataLake.AnalyticsJob -Pre
        Install-Package Microsoft.Azure.Management.DataLake.Store -Pre
        Install-Package Microsoft.Azure.Management.DataLake.StoreFileSystem -Pre
        Install-Package Microsoft.Azure.Management.DataLake.StoreUploader -Pre
        Install-Package WindowsAzure.Storage

4. 將新檔案新增至名為 **SampleUSQLScript.txt** 的專案，然後貼上下列 U-SQL 指令碼。 資料湖分析工作是以 U-SQL 語言撰寫。 若要深入了解 U SQL，請參閱 [開始 U SQL 語言使用](data-lake-analytics-u-sql-get-started.md) 和 [U SQL 語言參考](http://go.microsoft.com/fwlink/?LinkId=691348)。

     @searchlog =
         EXTRACT UserId          int,
                 Start           DateTime,
                 Region          string,
                 Query           string,
                 Duration        int?,
                 Urls            string,
                 ClickedUrls     string
         FROM "/Samples/Data/SearchLog.tsv"
         USING Extractors.Tsv();
    
     OUTPUT @searchlog   
         TO "/Output/SearchLog-from-Data-Lake.csv"
     USING Outputters.Csv();

 此 U-SQL 指令碼會使用 **Extractors.Tsv()** 讀取來源資料檔案，然後使用 **Outputters.Csv()** 建立 csv 檔案。

 在 C# 程式中，您必須準備 **/Samples/Data/SearchLog.tsv** 檔案及 **/Output/** 資料夾：

 使用儲存在預設資料湖帳戶中檔案的相對路徑，是比較容易的方法。 您也可以使用絕對路徑。 例如

     adl://<Data LakeStorageAccountName>.azuredatalakestore.net:443/Samples/Data/SearchLog.tsv

 您必須使用絕對路徑來存取連結的儲存體帳戶中的檔案。 儲存在連結 Azure 儲存體帳戶中之檔案的語法是：

     wasb://<BlobContainerName>@<StorageAccountName>.blob.core.windows.net/Samples/Data/SearchLog.tsv

 >[AZURE.NOTE] 目前不支援具有公用 Blob 或公用容器存取權限的 Azure Blob 容器。    

5. 在 Program.cs 中貼上下列程式碼：

     using System;
     using System.Security;
     using System.IO;
    
     using Microsoft.Azure;
     using Microsoft.Azure.Common.Authentication;
     using Microsoft.Azure.Common.Authentication.Models;
     using Microsoft.Azure.Management.DataLake.Store;
     using Microsoft.Azure.Management.DataLake.Store.Models;
     using Microsoft.Azure.Management.DataLake.StoreFileSystem;
     using Microsoft.Azure.Management.DataLake.StoreFileSystem.Models;
     using Microsoft.Azure.Management.DataLake.StoreUploader;
     using Microsoft.Azure.Common.Authentication.Factories;
    
     using Microsoft.Azure.Management.DataLake.Analytics;
     using Microsoft.Azure.Management.DataLake.Analytics.Models;
     using Microsoft.Azure.Management.DataLake.AnalyticsJob;
     using Microsoft.Azure.Management.DataLake.AnalyticsJob.Models;
    
     using Microsoft.WindowsAzure.Storage.Blob;
    
     namespace data_lake_analytics_get_started
     {
         class Program
         {
    
             private const string AzureSubscriptionID = "<Your Azure Subscription ID>";
    
             private const string ResourceGroupName = "<Existing Azure Resource Group Name>"; //See the Get started using portal article
             private const string Location = "<Azure Data Center>";  //For example, EAST US 2
             private const string DataLakeStoreAccountName = "<Data Lake Store Account Name>"; // The application will create this account.
             private const string DataLakeAnalyticsAccountName = "<Data Lake Analytics Account Name>"; //The application will create this account.
    
             private const string LocalFolder = @"C:\tutorials\downloads\";  //local folder with write permission for file transferring.
    
             private static DataLakeStoreManagementClient _dataLakeStoreClient;
             private static DataLakeStoreFileSystemManagementClient _dataLakeStoreFileSystemClient;
    
             private static DataLakeAnalyticsManagementClient _dataLakeAnalyticsClient;
             private static DataLakeAnalyticsJobManagementClient _dataLakeAnalyticsJobClient;
    
             static void Main(string[] args)
             {
                 var subscriptionId = new Guid(AzureSubscriptionID);
                 var _credentials = GetAccessToken();
    
                 _credentials = GetCloudCredentials(_credentials, subscriptionId);
                 _dataLakeStoreClient = new DataLakeStoreManagementClient(_credentials);
                 _dataLakeStoreFileSystemClient = new DataLakeStoreFileSystemManagementClient(_credentials);
                 _dataLakeAnalyticsClient = new DataLakeAnalyticsManagementClient(_credentials);
                 _dataLakeAnalyticsJobClient = new DataLakeAnalyticsJobManagementClient(_credentials);
    
                 //=========================
                 Console.WriteLine("Creating the Azure Data Lake Store account ...");
                 var storeAccountParameters = new DataLakeStoreAccountCreateOrUpdateParameters();
                 storeAccountParameters.DataLakeStoreAccount = new Microsoft.Azure.Management.DataLake.Store.Models.DataLakeStoreAccount
                 {
                     Name = DataLakeStoreAccountName,
                     Location = Location
                 };
                 _dataLakeStoreClient.DataLakeStoreAccount.Create(ResourceGroupName, storeAccountParameters);
    
                 //=========================
                 Console.WriteLine("Preparing the source data file ...");
    
                 // Download the source file from a public Azure Blob container.
                 CloudBlockBlob blob = new CloudBlockBlob(new Uri("https://adltutorials.blob.core.windows.net/adls-sample-data/SearchLog.tsv"));
                 blob.DownloadToFile(LocalFolder + "SearchLog.tsv", System.IO.FileMode.Create);
    
                 // Create a folder in the default Data Lake Store account.
                 _dataLakeStoreFileSystemClient.FileSystem.Mkdirs("/Samples/Data/", DataLakeStoreAccountName, "777");
    
                 // Upload the source file to the default Data Lake Store account
                 var uploadParameters = new UploadParameters(LocalFolder + "SearchLog.tsv", "/Samples/Data/SearchLog.tsv", DataLakeStoreAccountName, isOverwrite: true);
                 var uploader = new DataLakeStoreUploader(uploadParameters, new DataLakeStoreFrontEndAdapter(DataLakeStoreAccountName, _dataLakeStoreFileSystemClient));
                 uploader.Execute();
    
                 //=========================
                 Console.WriteLine("Creating the Data Lake Analytics account ...");
                 var analyticsAccountParameters = new DataLakeAnalyticsAccountCreateOrUpdateParameters();
                 analyticsAccountParameters.DataLakeAnalyticsAccount = new DataLakeAnalyticsAccount
                 {
                     Name = DataLakeAnalyticsAccountName,
                     Location = Location,
                     Properties = new DataLakeAnalyticsAccountProperties()
                 };
    
                 //create a DataLakeStoreAccount object, add it to the analytics client and then set it as the default ADL Store account
                 Microsoft.Azure.Management.DataLake.Analytics.Models.DataLakeStoreAccount storeAccountObject = new Microsoft.Azure.Management.DataLake.Analytics.Models.DataLakeStoreAccount();
                 storeAccountObject.Name = DataLakeStoreAccountName;
                 analyticsAccountParameters.DataLakeAnalyticsAccount.Properties.DataLakeStoreAccounts.Add(storeAccountObject);
                 analyticsAccountParameters.DataLakeAnalyticsAccount.Properties.DefaultDataLakeStoreAccount = DataLakeStoreAccountName;
    
                 _dataLakeAnalyticsClient.DataLakeAnalyticsAccount.Create(ResourceGroupName, analyticsAccountParameters);
    
                 //=========================
                 Console.WriteLine("Submitting the job ...");
    
                 USqlProperties uSQLProperties = new USqlProperties
                 {
                     Type = JobType.USql,
                     Script = File.ReadAllText("SampleUSQLScript.txt")
                 };
    
                 JobInformation jobParameters = new JobInformation
                 {
                     JobId = Guid.NewGuid(),
                     Name = "myfirstdatalakeanalyticsjob",
                     Properties = uSQLProperties,
                     Type = JobType.USql,
                     DegreeOfParallelism = 1,
                     Priority = 1
                 };
    
                 _dataLakeAnalyticsJobClient.Jobs.Create(ResourceGroupName, DataLakeAnalyticsAccountName, new JobInfoBuildOrCreateParameters { Job = jobParameters });
    
                 Console.WriteLine(" Downloading results ...");
                 FileCreateOpenAndAppendResponse beginOpenResponse = _dataLakeStoreFileSystemClient.FileSystem.BeginOpen("/Output/SearchLog-from-Data-Lake.csv", DataLakeStoreAccountName, new FileOpenParameters());
                 FileOpenResponse openResponse = _dataLakeStoreFileSystemClient.FileSystem.Open(beginOpenResponse.Location);
                 System.IO.File.WriteAllBytes(LocalFolder + "SearchLog-from-Data-Lake.csv", openResponse.FileContents);
    
                 Console.WriteLine("Done");
             }
    
             public static SubscriptionCloudCredentials GetAccessToken(string username = null, SecureString password = null)
             {
                 var authFactory = new AuthenticationFactory();
    
                 var account = new AzureAccount { Type = AzureAccount.AccountType.User };
    
                 if (username != null && password != null)
                     account.Id = username;
    
                 var env = AzureEnvironment.PublicEnvironments[EnvironmentName.AzureCloud];
                 return new TokenCloudCredentials(authFactory.Authenticate(account, env, AuthenticationFactory.CommonAdTenant, password, ShowDialog.Auto).AccessToken);
             }
    
             public static SubscriptionCloudCredentials GetCloudCredentials(SubscriptionCloudCredentials creds, Guid subId)
             {
                 return new TokenCloudCredentials(subId.ToString(), ((TokenCloudCredentials)creds).Token);
             }
         }
     }

7. 按 **F5** 鍵執行應用程式。

## 另請參閱

- 若要使用其他工具檢視同一個教學課程，請按一下頁面最上方的索引標籤選取器。
- 若要查看更 complexed 的查詢，請參閱 [使用 Azure 資料湖分析分析網站記錄檔](data-lake-analytics-analyze-weblogs.md)。
- 若要開始開發 U SQL 應用程式，請參閱 [開發 U SQL 指令碼使用資料湖 Tools for Visual Studio](data-lake-analytics-data-lake-tools-get-started.md)。
- 若要了解 U SQL，請參閱 [開始使用 Azure 資料湖分析 U-SQL 語言](data-lake-analytics-u-sql-get-started.md)。
- 管理工作，請參閱 [管理 Azure 資料湖分析使用 Azure 入口網站](data-lake-analytics-manage-use-portal.md)。
- 若要取得資料湖分析的概觀，請參閱 [Azure 資料湖分析概觀](data-lake-analytics-overview.md)。





